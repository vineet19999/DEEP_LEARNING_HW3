{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import tensorboard #pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, MarianTokenizer, MarianMTModel,DistilBertTokenizer, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006102561950683594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)olve/main/source.spm",
       "rate": null,
       "total": 778395,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14a2906d98146d7b7cf53e5789ff08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009053945541381836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)olve/main/target.spm",
       "rate": null,
       "total": 802397,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e135ad3345042b2aa94b88a6eb72d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0051364898681640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)olve/main/vocab.json",
       "rate": null,
       "total": 1339166,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6d17c525c04d658a010e027339d5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004270792007446289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 42,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e7e1b4c1c74014a5ccfcd73021460b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007335662841796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)lve/main/config.json",
       "rate": null,
       "total": 1416,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bbe7c16bbf4ccf9339ae8ab2860509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0034656524658203125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 300827685,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28139fbe93a4543a7b75ffa6de22dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019795656204223633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 293,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dae16cb5f73498cb6d857b3bc14394e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00750732421875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)lve/main/config.json",
       "rate": null,
       "total": 629,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800db4dcc8b547ba9e032b1e6aecafe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004181623458862305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 267832558,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaa8fc5bc1b442689a63da6a1ab9659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007189750671386719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)solve/main/vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59c73179190483ab7196fc0337a0c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006217002868652344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 48,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45a7eb3992f4d698bfacd70939721b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/lkk/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /home/lkk/.cache/torch_extensions/py310_cu118/cuda_kernel...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/lkk/.cache/torch_extensions/py310_cu118/cuda_kernel/build.ninja...\n",
      "Building extension module cuda_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] /home/lkk/miniconda3/envs/mypy310/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/TH -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/THC -isystem /home/lkk/miniconda3/envs/mypy310/include -isystem /home/lkk/miniconda3/envs/mypy310/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/kernels/mra/cuda_kernel.cu -o cuda_kernel.cuda.o \n",
      "[2/4] c++ -MMD -MF torch_extension.o.d -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/TH -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/THC -isystem /home/lkk/miniconda3/envs/mypy310/include -isystem /home/lkk/miniconda3/envs/mypy310/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/kernels/mra/torch_extension.cpp -o torch_extension.o \n",
      "[3/4] /home/lkk/miniconda3/envs/mypy310/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/TH -isystem /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/include/THC -isystem /home/lkk/miniconda3/envs/mypy310/include -isystem /home/lkk/miniconda3/envs/mypy310/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/kernels/mra/cuda_launch.cu -o cuda_launch.cuda.o \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load CUDA kernels. Mra requires custom CUDA kernels. Please verify that compatible versions of PyTorch and CUDA Toolkit are installed: Error building extension 'cuda_kernel'\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/4] c++ cuda_kernel.cuda.o cuda_launch.cuda.o torch_extension.o -shared -L/home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/lkk/miniconda3/envs/mypy310/lib64 -lcudart -o cuda_kernel.so\n",
      "\u001b[31mFAILED: \u001b[0mcuda_kernel.so \n",
      "c++ cuda_kernel.cuda.o cuda_launch.cuda.o torch_extension.o -shared -L/home/lkk/miniconda3/envs/mypy310/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/lkk/miniconda3/envs/mypy310/lib64 -lcudart -o cuda_kernel.so\n",
      "/usr/bin/ld: cannot find -lcudart: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "ninja: build stopped: subcommand failed.\n"
     ]
    }
   ],
   "source": [
    "#This code translates user input to french and gets the sentiment at the same time, it incorporates translation and sentiment analysis.\n",
    "\n",
    "#using pre-trained translation model\n",
    "translation_model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translation_tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(translation_model_name)\n",
    "\n",
    "#Using sentiment model\n",
    "#sentiment_analysis_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentiment_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_analysis_pipeline = pipeline(\"sentiment-analysis\", model=sentiment_model, tokenizer=sentiment_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! i can help you translate to french, please enter a sentence?\n",
      "Chatbot: Sentiment: POSITIVE. Translated text in French: Bonjour, bonjour.\n",
      "Chatbot: Sentiment: POSITIVE. Translated text in French: Bonjour Français, bonjour.\n",
      "Chatbot: Sentiment: POSITIVE. Translated text in French: J'aime les hamburgers.\n",
      "Chatbot: Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot: Hello! i can help you translate to french, please enter a sentence?\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "#checks exit functionality : chatbot stops when user uses following in a sentence\n",
    "    if any(token in user_input.lower() for token in ['exit', 'bye']):\n",
    "        print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "        break\n",
    "\n",
    " # Perform sentiment analysis on English input\n",
    "    sentiment_output = sentiment_analysis_pipeline(user_input)[0]\n",
    "    sentiment_label = sentiment_output['label']\n",
    "\n",
    "  # Translate English input to French\n",
    "    translated_input = translation_tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "    translated_output = translation_model.generate(translated_input, max_length=100, num_return_sequences=1)\n",
    "    translated_text = translation_tokenizer.decode(translated_output[0], skip_special_tokens=True)\n",
    "\n",
    "  # Display chatbot response\n",
    "    response = f\"Chatbot: Sentiment: {sentiment_label}. Translated text in French: {translated_text}\"\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Initialising conversation history\n",
    "conversation_history = []\n",
    "\n",
    "print(\"Chatbot: Hello! How can I assist you today?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: It sounds like you're feeling positive! How can I help you further?\n",
      "Chatbot: Sure! Please provide the city name:\n",
      "Chatbot: I'm sorry, I couldn't fetch the weather information for that city.\n",
      "Chatbot: It sounds like you're feeling positive! How can I help you further?\n",
      "Chatbot: Sure! Please provide the city name:\n",
      "Chatbot: I'm sorry, I couldn't fetch the weather information for that city.\n",
      "Chatbot: Sure! Please provide the city name:\n",
      "Chatbot: The weather in san jose is currently broken clouds with a temperature of 23.34°C.\n",
      "Chatbot: Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "  #exit mechanism\n",
    "    if any(token in user_input.lower() for token in ['exit', 'bye']):\n",
    "        print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "        break\n",
    "\n",
    "    if 'weather' in user_input.lower():\n",
    "        print(\"Chatbot: Sure! Please provide the city name:\")\n",
    "        city = input(\"You: \")\n",
    "        weather_url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
    "        response = requests.get(weather_url).json()\n",
    "        if response.get('main') and response.get('weather'):\n",
    "            temperature = response['main']['temp']\n",
    "            description = response['weather'][0]['description']\n",
    "            response = f\"Chatbot: The weather in {city} is currently {description} with a temperature of {temperature:.2f}°C.\"\n",
    "        else:\n",
    "            response = \"Chatbot: I'm sorry, I couldn't fetch the weather information for that city.\"\n",
    "        print(response)  # Print the weather response\n",
    "    else:\n",
    "        sentiment = sentiment_analysis_pipeline(user_input)[0]\n",
    "        sentiment_label = sentiment['label']\n",
    "        sentiment_score = sentiment['score']\n",
    "\n",
    "        if sentiment_label == 'POSITIVE':\n",
    "            response = \"Chatbot: It sounds like you're feeling positive! How can I help you further?\"\n",
    "        elif sentiment_label == 'NEGATIVE':\n",
    "            response = \"Chatbot: I'm sorry to hear that. How can I assist you in improving your mood?\"\n",
    "        else:\n",
    "            response = \"Chatbot: Thanks for sharing. Is there something specific you'd like to discuss?\"\n",
    "\n",
    "        conversation_history.append({\"user_input\": user_input, \"sentiment\": sentiment_label, \"chatbot_response\": response})\n",
    "        print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
