{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE score: Standing for Recall-Oriented Understudy for Gisting Evaluation\n",
    "Text summarization is the art of distilling the essence of text content. It involves creating coherent and concise summaries of longer documents while retaining key information. The ROUGE Score is the go-to metric for assessing the quality of such machine-generated summaries.\n",
    "\n",
    "pip install rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROUGE Score has three main components: ROUGE-N, ROUGE-L, and ROUGE-S\n",
    "\n",
    "ROUGE-N\n",
    "ROUGE-N is a component of the ROUGE score that quantifies the overlap of N-grams, contiguous sequences of N items (typically words or characters), between the system-generated summary and the reference summary. It provides insights into the precision and recall of the system's output by considering the matching N-gram sequences.\n",
    "\n",
    "ROUGE-L\n",
    "ROUGE-L, another component of the ROUGE Score, calculates the Longest Common Subsequence (LCS) between the system and reference summaries. Unlike N-grams, LCS measures the maximum sequence of words (not necessarily contiguous) that appear in both summaries. It offers a more flexible similarity measure and helps capture shared information beyond strict word-for-word matches.\n",
    "\n",
    "ROUGE-S\n",
    "ROUGE-S focuses on skip-bigrams. A skip-bigram is a pair of words in a sentence that allows for gaps or words in between. This component identifies the skip-bigram overlap between the system and reference summaries, enabling the assessment of sentence-level structure similarity. It can capture paraphrasing relationships between sentences and provide insights into the system's ability to convey information with flexible word ordering.\n",
    "\n",
    "ref: https://thepythoncode.com/article/calculate-rouge-score-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to initialize the RougeScorer. You need to specify which types of ROUGE scores you are interested in. For instance, if you want to calculate ROUGE-1, ROUGE-2, and ROUGE-L scores, you would initialize the scorer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923)\n",
      "rouge2: Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272)\n",
      "rougeL: Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923)\n"
     ]
    }
   ],
   "source": [
    "candidate_summary = \"the cat was found under the bed\"\n",
    "reference_summary = \"the cat was under the bed\"\n",
    "scores = scorer.score(reference_summary, candidate_summary)\n",
    "for key in scores:\n",
    "    print(f'{key}: {scores[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:\n",
      "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.7142857142857143, recall=0.8333333333333334, fmeasure=0.7692307692307692)]\n",
      "rouge2:\n",
      "[Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272), Score(precision=0.3333333333333333, recall=0.4, fmeasure=0.3636363636363636)]\n",
      "rougeL:\n",
      "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)]\n"
     ]
    }
   ],
   "source": [
    "candidate_summary = \"the cat was found under the bed\"\n",
    "reference_summaries = [\"the cat was under the bed\", \"found a cat under the bed\"]\n",
    "scores = {key: [] for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "for ref in reference_summaries:\n",
    "    temp_scores = scorer.score(ref, candidate_summary)\n",
    "    for key in temp_scores:\n",
    "        scores[key].append(temp_scores[key])\n",
    "\n",
    "for key in scores:\n",
    "    print(f'{key}:\\n{scores[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5984da253e4b959e8aa74643e5d063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923,\n",
       " 'rougeLsum': 0.923076923076923}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923,\n",
       " 'rougeLsum': 0.923076923076923}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.8333333333333333,\n",
       " 'rouge2': 0.5,\n",
       " 'rougeL': 0.8333333333333333,\n",
       " 'rougeLsum': 0.8333333333333333}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\n",
    "results = rouge.compute(predictions=predictions,\n",
    "                        references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "predictions (list): list of predictions to score. Each prediction should be a string with tokens separated by spaces.\n",
    "references (list or list[list]): list of reference for each prediction or a list of several references per prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': [1.0, 0.6666666666666666],\n",
       " 'rouge2': [1.0, 0.0],\n",
       " 'rougeL': [1.0, 0.6666666666666666],\n",
       " 'rougeLsum': [1.0, 0.6666666666666666]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [[\"hello there\"], [\"kenobi\", \"general\"]]\n",
    "results = rouge.compute(predictions=predictions,\n",
    "                        references=references, use_aggregator=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "localscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0),\n",
       " 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0),\n",
       " 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newscores = localscorer.score(references[0][0], predictions[0])\n",
    "newscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923),\n",
       " 'rouge2': Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272),\n",
       " 'rougeL': Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newscores = localscorer.score(reference_summary, generated_summary)\n",
    "newscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newscores['rouge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923076923076923"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newscores['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
       " 'rouge2': Score(precision=0.2857142857142857, recall=0.25, fmeasure=0.26666666666666666),\n",
       " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/google-research/google-research/tree/master/rouge\n",
    "scores = localscorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0),\n",
       " 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
       " 'rougeL': Score(precision=0.5, recall=0.5, fmeasure=0.5)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = localscorer.score_multi([\"first text\", \"first something\"], \"text first\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_summary = [\"the cat was found under the bed\",\"I absolutely loved reading the Hunger Games\"]\n",
    "reference_summaries = [\"the cat was under the bed\", \"I loved reading the Hunger Games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "newscores = localscorer.score_multi(reference_summaries[0], candidate_summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
       " 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
       " 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat was under the bed'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923),\n",
       " 'rouge2': Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272),\n",
       " 'rougeL': Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newscores = localscorer.score(reference_summaries[0], candidate_summary[0])\n",
    "newscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets actually computes confidence intervals for precision, recall, and F1-score; these are the low, mid, and high attributes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
