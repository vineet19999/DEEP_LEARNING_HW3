{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMT21 Translation\n",
    "\n",
    "https://huggingface.co/facebook/wmt21-dense-24-wide-x-en\n",
    "\n",
    "WMT 21 X-En is a 4.7B multilingual encoder-decoder (seq-to-seq) model trained for one-to-many multilingual translation. It was introduced in this paper and first released in this repository.\n",
    "\n",
    "The model can directly translate text from 7 languages: Hausa (ha), Icelandic (is), Japanese (ja), Czech (cs), Russian (ru), Chinese (zh), German (de) to English.\n",
    "\n",
    "To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.\n",
    "\n",
    "Since the model was trained with domain tags, you should prepend them to the input as well.\n",
    "\n",
    "\"wmtdata newsdomain\": Use for sentences in the news domain\n",
    "\"wmtdata otherdomain\": Use for sentences in all other domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "#https://huggingface.co/facebook/wmt21-dense-24-wide-en-x\n",
    "model_name=\"facebook/wmt21-dense-24-wide-en-x\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, tokenizer, textinput):\n",
    "    newsdomain=\"wmtdata newsdomain \"\n",
    "    otherdomain=\"wmtdata otherdomain \"\n",
    "    textinput = newsdomain+textinput\n",
    "    inputs = tokenizer(textinput, return_tensors=\"pt\")\n",
    "\n",
    "    # translate English to Chinese\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\")) #max_new_tokens\n",
    "    result=tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以色列军队搜查加沙的希法医院,寻找哈马斯存在的证据。']\n"
     ]
    }
   ],
   "source": [
    "textinput=\"Israeli troops scour Gaza’s al-Shifa Hospital for evidence of Hamas.\"\n",
    "result = translate(model, tokenizer, textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以色列对加沙地带最大医院的突袭行动于周四进入第二天,以军搜查了哈马斯在该医院地下拥有大量基础设施的证据,以色列和美国官员称这些基础设施位于该设施之下。以色列国防军周四表示,搜查行动在医院附近的一所房屋中发现了一名被俘以色列妇女的尸体,以及武器。周三,以色列国防军发布了据称属于哈马斯的小型武器藏匿处的照片和视频。军方周四在其案例中添加了一张照片和一段视频,该照片和视频显示了一个粗糙的空洞,该空洞被描述为\"操作隧道竖井\"。《华盛顿邮报》核实了该竖井在al-Shifa医院建筑群内的位置,但无法核实该开口通向何处,也无法核实其目的。']\n"
     ]
    }
   ],
   "source": [
    "textinput=\"The Israeli raid of the Gaza Strip’s largest hospital stretched into its second day Thursday as troops searched for evidence of the extensive Hamas infrastructure that Israeli and U.S. officials have said lies beneath the facility. The Israel Defense Forces said Thursday that searches had uncovered the body of a captive Israeli woman in a house near the hospital, along with weapons. On Wednesday, the IDF released photographs and video of small caches of weapons it said belonged to Hamas. The military added to its case Thursday with a photo and video of a rough cavity that it described as an “operational tunnel shaft.” The Washington Post verified the location of the shaft inside the al-Shifa Hospital complex but could not verify where the opening led or what its purpose might be.\"\n",
    "result = translate(model, tokenizer, textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): Embedding(128009, 2048, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): Embedding(128009, 2048, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "          (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): Embedding(128009, 2048, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "          (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128009, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='mps'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, tokenizer, textinput, device):\n",
    "    newsdomain=\"wmtdata newsdomain \"\n",
    "    otherdomain=\"wmtdata otherdomain \"\n",
    "    textinput = newsdomain+textinput\n",
    "    inputs = tokenizer(textinput, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "\n",
    "    # translate English to Chinese\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\")) #max_new_tokens\n",
    "    result=tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/utils.py:690: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686130/work/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/beam_search.py:358: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686130/work/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  sent_lengths_max = sent_lengths.max().item() + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以色列对加沙地带最大医院的突袭行动于周四进入第二天,以军搜查了哈马斯在该医院地下拥有大量基础设施的证据,以色列和美国官员称这些基础设施位于该设施之下。以色列国防军周四表示,搜查行动在医院附近的一所房屋中发现了一名被俘以色列妇女的尸体,以及武器。周三,以色列国防军发布了据称属于哈马斯的小型武器藏匿处的照片和视频。军方周四在其案例中添加了一张照片和一段视频,该照片和视频显示了一个粗糙的空洞,该空洞被描述为\"操作隧道竖井\"。《华盛顿邮报》核实了该竖井在al-Shifa医院建筑群内的位置,但无法核实该开口通向何处,也无法核实其目的。']\n"
     ]
    }
   ],
   "source": [
    "textinput=\"The Israeli raid of the Gaza Strip’s largest hospital stretched into its second day Thursday as troops searched for evidence of the extensive Hamas infrastructure that Israeli and U.S. officials have said lies beneath the facility. The Israel Defense Forces said Thursday that searches had uncovered the body of a captive Israeli woman in a house near the hospital, along with weapons. On Wednesday, the IDF released photographs and video of small caches of weapons it said belonged to Hamas. The military added to its case Thursday with a photo and video of a rough cavity that it described as an “operational tunnel shaft.” The Washington Post verified the location of the shaft inside the al-Shifa Hospital complex but could not verify where the opening led or what its purpose might be.\"\n",
    "result = translate(model, tokenizer, textinput, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Tagalog\n",
    "\n",
    "https://github.com/YoonjungChoi/ProjectHAWAII/blob/main/Helsinki-NLP_TEST.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7180f585314c5aa64b089be2643ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad2331d3bae4d2e8fbf4d1b96ee23a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ba695a98d947bcacbb859ce8e29a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading source.spm:   0%|          | 0.00/827k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bec51b3508a48ef903c5aaf4cad7f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading target.spm:   0%|          | 0.00/835k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d656e0c59bf4b5e9f4ae84e1ff77862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa373b940a71410088b2b583fe901d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/296M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2333491d1ec247838e9b7743bb909cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Paglubog sa pinalawak na mga sinulid'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#https://huggingface.co/Helsinki-NLP\n",
    "'''\n",
    "Pipeline supports running on CPU or GPU through the device argument.\n",
    "Users can specify device argument as an integer, -1 meaning “CPU”, >= 0 referring the CUDA device ordinal.\n",
    "'''\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-tl\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2667,  1534, 49919,    12, 15147, 26102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textinput=\"Default to expanded threads\"\n",
    "inputs = tokenizer(textinput, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(57373, 512, padding_idx=57372)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(57373, 512, padding_idx=57372)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(57373, 512, padding_idx=57372)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=57373, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57372,   508, 20428,     4, 15728,     6,    10, 18531,     0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = model.generate(\n",
    "                        inputs[\"input_ids\"],\n",
    "                        attention_mask=inputs[\"attention_mask\"],\n",
    "                        max_length=128,\n",
    "                    )\n",
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paglubog sa pinalawak na mga sinulid']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990165,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 46.750469682990165\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "#bleu = sacrebleu.corpus_bleu(predictions, references, tokenize=\"none\", lowercase=True)\n",
    "bleu = sacrebleu.corpus_bleu(predictions, references)\n",
    "print(\"BLEU score:\", bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(list(results.keys()))\n",
    "print(round(results[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "39.8\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"hello there general kenobi\", \"on our way to ankh morpork\"]\n",
    "references = [[\"hello there general kenobi\", \"hello there !\"], [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
    "results = metric.compute(predictions=predictions, references=references, tokenize=\"none\")\n",
    "print(list(results.keys()))\n",
    "print(round(results[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 39.76353643835252,\n",
       " 'counts': [6, 4, 2, 1],\n",
       " 'totals': [10, 8, 6, 4],\n",
       " 'precisions': [60.0, 50.0, 33.333333333333336, 25.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 10,\n",
       " 'ref_len': 7}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\"predictions\": predictions, \"references\": references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': ['hello there general kenobi', 'on our way to ankh morpork'],\n",
       " 'references': [['hello there general kenobi', 'hello there !'],\n",
       "  ['goodbye ankh morpork', 'ankh morpork']]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: BLEU = 39.76 60.0/50.0/33.3/25.0 (BP = 1.000 ratio = 1.429 hyp_len = 10 ref_len = 7)\n"
     ]
    }
   ],
   "source": [
    "bleu = sacrebleu.corpus_bleu(predictions, references)\n",
    "print(\"BLEU score:\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.76353643835252"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4, 2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 8, 6, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60.0, 50.0, 33.333333333333336, 25.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.sys_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=['\"我将呼吁上帝,谁值得被赞美。\"', '母亲母亲、', '给你']\n",
    "refs=[['我要求告当赞美的耶和华'], ['媳妇'], ['給你錢']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 5.412989186545263\n"
     ]
    }
   ],
   "source": [
    "bleu = sacrebleu.corpus_bleu(preds, refs, tokenize=\"zh\")\n",
    "print(\"BLEU score:\", bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 100.00000000000004\n"
     ]
    }
   ],
   "source": [
    "preds=['我将呼吁上帝,谁值得被赞美', '母亲母亲、', '给你']\n",
    "#refs=[['我将呼吁上帝,谁值得被赞美'], ['母亲母亲、'], ['给你']]\n",
    "refs=[['我将呼吁上帝,谁值得被赞美', '母亲母亲、', '给你']]\n",
    "bleu = sacrebleu.corpus_bleu(preds, refs, tokenize=\"zh\")\n",
    "print(\"BLEU score:\", bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "result=metric.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 49.19195660047277\n"
     ]
    }
   ],
   "source": [
    "refs = [ # First set of references\n",
    "        ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
    "        # Second set of references\n",
    "        ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.'],\n",
    "        ]\n",
    "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
    "bleu = sacrebleu.corpus_bleu(sys, refs, tokenize=\"none\")\n",
    "print(\"BLEU score:\", bleu.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
