{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#hfhome_dir=\"/data/cmpe249-fa23/Huggingfacecache\"\n",
    "hfhome_dir=os.path.join('D:',os.sep, 'Cache','huggingface')\n",
    "#os.environ['TRANSFORMERS_CACHE'] = hfhome_dir\n",
    "os.environ['HF_HOME'] = hfhome_dir\n",
    "#os.environ['HF_HUB_CACHE'] = os.path.join(hfhome_dir, 'hub')\n",
    "#os.environ['HF_DATASETS_CACHE'] = hfhome_dir\n",
    "#HF_HUB_OFFLINE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "dataset_split = load_dataset(\n",
    "        \"librispeech_asr\", #'librispeech_asr'\n",
    "        'clean',\n",
    "        split='train.100',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "    num_rows: 28539\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iter = iter(dataset_split)\n",
    "sample = next(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:\\\\Users\\\\lkk68\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\a77e60630727409949078da012aba02689008a522ed19f5f43300c735482710c\\\\374-180298-0000.flac',\n",
       " 'audio': {'path': 'C:\\\\Users\\\\lkk68\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\a77e60630727409949078da012aba02689008a522ed19f5f43300c735482710c\\\\374-180298-0000.flac',\n",
       "  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,\n",
       "         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',\n",
       " 'speaker_id': 374,\n",
       " 'chapter_id': 180298,\n",
       " 'id': '374-180298-0000'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232480\n"
     ]
    }
   ],
   "source": [
    "audioarray=sample[\"audio\"][\"array\"]\n",
    "print(len(audioarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"audio\"][\"sampling_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 1.59MB/s]\n",
      "model.safetensors: 100%|██████████| 378M/378M [00:03<00:00, 116MB/s]  \n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100%|██████████| 163/163 [00:00<?, ?B/s] \n",
      "vocab.json: 100%|██████████| 291/291 [00:00<00:00, 292kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 84.9kB/s]\n",
      "preprocessor_config.json: 100%|██████████| 159/159 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "# import model, feature extractor, tokenizer\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward sample through model to get greedily predicted transcription ids\n",
    "processedsample=feature_extractor(sample[\"audio\"][\"array\"],sampling_rate=sample[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[ 0.0117,  0.0122,  0.0122,  ..., -0.0041, -0.0026, -0.0001]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = processedsample.input_values\n",
    "logits = model(input_values).logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([726, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19, 19, 11, 11,  0,  0,  0,\n",
       "         0,  7,  0, 23,  0,  0,  0,  6,  0,  0,  5,  5, 13, 13,  0,  4,  4,  4,\n",
       "         0,  0,  0,  0,  0, 12,  0,  0,  0, 10, 10,  0, 28,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  6,  6,  0,  0,  5,  0,  0,  0,  0,  0,  0,\n",
       "         5,  0,  9,  9,  0,  0,  0,  4,  4,  4,  4,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  4,  4,  4, 17, 17,  0,  0,\n",
       "        10, 21, 11, 11,  6,  6,  0,  4,  4,  0, 11,  0,  7, 25, 25,  5,  4,  4,\n",
       "         0,  0,  6,  0,  0,  0,  8,  8, 15, 15,  0, 14, 14,  4,  4,  4, 22, 22,\n",
       "         0,  8, 16, 16,  0,  4,  4,  4,  4,  8, 20, 20,  4,  4,  4,  6,  6, 11,\n",
       "         5,  5,  4,  4, 24, 24,  0,  5,  5,  0, 21, 21,  0,  0, 10,  9,  9,  0,\n",
       "         0,  9,  9,  0, 10,  9,  9, 21, 21,  4,  4,  4,  0,  8, 20,  4,  4,  4,\n",
       "         6, 11, 11,  0, 10,  0, 12, 12,  4,  4,  0,  0,  0,  0, 15, 15,  0,  0,\n",
       "         0, 10,  0,  0,  0,  0,  7,  0, 10, 10,  0,  0,  0,  0, 12,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  8,  0,  0,  9,  9,  0,  4,  4,  4,  4,\n",
       "         4,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  9,  0,  0,  4,  4,  7,  4,\n",
       "         4,  4,  0, 20, 20,  0,  0,  0,  0,  0,  5, 18,  0,  0,  4,  4,  4,  0,\n",
       "        15, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  9,  9,  0,  5,\n",
       "         0,  0,  0,  0, 12, 12,  0,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0, 24,  0,  0, 16,  6,  0,  4,  4,  0, 10,  0,  4,  4,  0, 18,\n",
       "        18,  0,  7,  9,  9,  0,  0,  6,  6,  0,  0,  5, 14, 14,  4,  4, 22, 22,\n",
       "         8, 16,  0,  0,  4,  4,  6,  0,  8,  4,  4,  0,  0, 12,  0,  0,  5,  0,\n",
       "         0,  0,  5,  0,  4,  4,  4,  4,  0,  0,  0,  0,  0,  5,  5, 25, 25,  0,\n",
       "         5, 13, 13, 13,  0,  0, 22,  0,  0,  4,  4,  0,  0, 12, 12,  0,  0,  6,\n",
       "         0,  0,  0,  0,  5,  0,  0,  0, 23,  0,  0,  4,  4,  0, 24,  0,  0,  0,\n",
       "         0, 22,  0,  4,  4, 18, 18, 11, 11, 10,  0, 19, 11, 11,  4,  4,  4,  0,\n",
       "         0, 18,  0,  0,  5,  0,  0,  4,  0,  0, 19,  0,  0,  0,  0,  0,  0,  0,\n",
       "         7,  0,  0, 17,  0,  5,  5,  4,  4,  4,  4,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  4,  4,  4,  0,  0,\n",
       "         6,  0,  8,  0,  0,  0,  8,  8,  0,  4,  4,  4,  0,  7,  7,  0, 21,  0,\n",
       "        13, 13,  0,  5,  0,  0,  5,  5,  0,  4,  4,  6,  6,  0,  8,  0,  4,  4,\n",
       "        18, 11, 11,  0,  7,  6,  0,  0,  0,  0,  5,  0, 25,  0,  0,  5,  5, 13,\n",
       "         0,  4,  4,  4, 17, 17,  0,  0,  0,  7, 13, 13,  0, 21, 21, 16, 16,  5,\n",
       "         5,  0, 13, 13,  0, 10, 10,  0,  0,  6,  5,  5,  4,  4,  0, 18, 18,  0,\n",
       "         0,  0,  0, 10,  0, 12,  0, 11, 11, 11,  0,  5,  5, 14, 14,  0,  4,  4,\n",
       "         4,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids = torch.argmax(logits, axis=-1)\n",
    "pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([726])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve word stamps (analogous commands for `output_char_offsets`)\n",
    "outputs = tokenizer.decode(pred_ids, output_word_offsets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n",
    "time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_offsets = [\n",
    "    {\n",
    "        \"word\": d[\"word\"],\n",
    "        \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n",
    "        \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n",
    "    }\n",
    "    for d in outputs.word_offsets\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'CHAPTER', 'start_time': 0.22, 'end_time': 0.64},\n",
       " {'word': 'SIXTEEN', 'start_time': 0.82, 'end_time': 1.52},\n",
       " {'word': 'I', 'start_time': 3.42, 'end_time': 3.44}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_offsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_split.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2ForCTC\n",
    "# transcribe speech\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(pred_ids)\n",
    "transcription[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.74"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "# compute loss\n",
    "loss = model(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Wav2Vec2 for English ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = dataset_split\n",
    "validation_split_percentage = 1\n",
    "num_validation_samples = raw_datasets[\"train\"].num_rows * validation_split_percentage // 100\n",
    "\n",
    "valkeyname=\"test\"\n",
    "raw_datasets[valkeyname] = raw_datasets[\"train\"].select(range(num_validation_samples))\n",
    "raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(num_validation_samples, raw_datasets[\"train\"].num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 28254\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.remove_columns([\"speaker_id\", \"chapter_id\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 28254\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIGHT HAVE SEEN THAT THE CHIN WAS VERY POINTED AND PRONOUNCED THAT THE BIG EYES WERE FULL OF SPIRIT AND VIVACITY THAT THE MOUTH WAS SWEET LIPPED AND EXPRESSIVE THAT THE FOREHEAD WAS BROAD AND FULL IN SHORT OUR DISCERNING EXTRAORDINARY OBSERVER MIGHT HAVE CONCLUDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE OTHER RETIRING BACKWARD AND BOWING WITH HIS HANDS ON HIS KNEES REPLIED INDEED INDEED I AM FILLED WITH ADMIRATION AT THE GOODNESS OF YOUR HEART WHEN I HEAR YOU SPEAK THUS I FEEL MORE THAN EVER HOW GREAT IS THE LOVE I BEAR YOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SO ADAM GOT ON HIS HORSE AGAIN AND RODE TO THE TOWN PUTTING UP AT THE OLD INN AND TAKING A HASTY DINNER THERE IN THE COMPANY OF THE TOO CHATTY LANDLORD FROM WHOSE FRIENDLY QUESTIONS AND REMINISCENCES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUT THE KNOWLEDGE ONLY ANGERED HER FURTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXCEPT JERMYN HILLIARD JUNIOR AND HE FOLLOWED ME RIGHT INTO THE PARLOR CAR AND SAT DOWN IN THE CHAIR EXACTLY OPPOSITE PATTY THEY CRIED IN SHOCKED CHORUS YOU SURELY DIDN'T SPEAK TO HIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I THEREFORE BOUND IT CLOSELY TO HIS SIDE AND ASKED HIM IF IN HIS EXHAUSTED AND TREMBLING CONDITION HE WAS STILL ABLE TO WALK YES HE BRAVELY REPLIED SO WITH A STEADYING ARM AROUND HIM AND MANY STOPS FOR REST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INTO MY OWN BERKSHIRE HILLS THAT SIT ALL SADLY GUARDING THE GATES OF MASSACHUSETTS UP THE STAIRS I RAN TO THE WAN MOTHER AND WHIMPERING BABE TO THE SANCTUARY ON WHOSE ALTAR A LIFE AT MY BIDDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AND IN A WORD PRESSED ME TO BUY BETTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BUT HER MIND WAS VERY FAR FROM THE CORRESPONDENCE BEFORE HER SHE HEARD THE SOFT THUD OF THE FRONT DOOR CLOSING AND RISING SHE CROSSED THE ROOM RAPIDLY AND LOOKED DOWN THROUGH THE WINDOW TO THE STREET SHE WATCHED FISHER UNTIL HE WAS OUT OF SIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RAPIDLY THE AIRSHIP ASCENDED AND WHEN IT WAS HIGH OVER THE TOWN OF SHOPTON TOM HEADED THE CRAFT DUE WEST LOOKING DOWN HE TRIED TO DESCRY MARY NESTOR IN HER CARRIAGE BUT THE TREES WERE IN THE WAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OF WHICH THEY INTENDED TO TAKE ADVANTAGE TO SUCH DEEP PERFIDY TO SUCH UNBOUNDED USURPATIONS IT WAS NECESSARY TO OPPOSE A PROPER FIRMNESS AND RESOLUTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WITH A PLEASING FAINT AROMA AND TASTE AND A QUALITY OF IMMEDIATE SUPPORT AND STIMULUS HE PUT DOWN THE VESSEL AND LOOKED ABOUT HIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DO YOU SAY THAT WE ENDURED GREAT HARDSHIPS THAT DEPENDS UPON THE POINT OF VIEW AS TO THIS RETURN TRIP I CAN TRULY SAY FOR MYSELF THAT IT WAS NOT ONE OF HARDSHIP I ENJOYED OVERCOMING THE DIFFICULTIES AND SO DID THE GREATER NUMBER OF THE COMPANY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HERE I FEEL UNEASY ABOUT THE NAME OF THIS LILY FOR THE COMPOSITORS HAVE A PERVERSE TRICK OF MAKING ME SAY ALL KINDS OF ABSURD THINGS WHOLLY UNWARRANTED BY PLAIN COPY AND I FEAR THAT THE LILY OF SAN PITCH WILL APPEAR IN PRINT AS THE WIDOW OF SAM PATCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DIFFERING FROM THEM ONLY IN RESPECT TO INTENSITY AND FORCE BUT HE WAS A CLEVER MAN WITH IDEAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WELL BECOMING A PRIEST OF THE GENTLE JESUS OR IF HE FANCIED HE MUST SPEAK OF CONFESSION WHY DID HE NOT SPEAK OF IT IN PLAIN HONEST TERMS INSTEAD OF SUGGESTING THE IDEA OF IT SO THAT THE POOR BOY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AND CONVINCED OF HER GREAT LOVE TOWARDS HIMSELF HAD FORGOTTEN ANY PREVIOUS JEALOUSY OF PHILIP SECURE AND EXULTANT HIS BROAD HANDSOME WEATHER BRONZED FACE WAS AS GREAT A CONTRAST TO PHILIP'S LONG THOUGHTFUL SALLOW COUNTENANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HE IS SPLENDID BIG SIX FOOTER WITH MAGNIFICENT MUSCLES RED CHEEKS AND CURLY YELLOW HAIR I CAN'T SEE HOW HE CAN BE CONTENTED TO SIT DOWN AND TEACH MUSHY ENGLISH LITERATURE AND POETRY AND THAT SORT OF THING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I INTEND INCREASING THAT TO FIVE POUNDS BECAUSE YOU SUIT ME MOST ADMIRABLY THANK YOU SAID THE GIRL QUIETLY BUT I AM ALREADY BEING PAID QUITE SUFFICIENT SHE LEFT HIM A LITTLE ASTONISHED AND NOT A LITTLE RUFFLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TRULY WOULD HE OF HIMSELF CONSTITUTE THE MUCH HERALDED YELLOW PERIL WERE IT NOT FOR HIS PRESENT MANAGEMENT THIS MANAGEMENT HIS GOVERNMENT IS SET CRYSTALLIZED IT IS WHAT BINDS HIM DOWN TO BUILDING AS HIS FATHERS BUILT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"].remove_columns([\"file\", 'audio']), num_examples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE JUST HAD TO FIND OUT IF THERE WAS ANYTHING NEW OVER THERE HARDLY HAD HE REACHED IT WHEN HE HEARD A PLAINTIVE VOICE CRYING PEE WEE PEE WEE PEE WEE PETER CHUCKLED HAPPILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUT HE DOES NOT PERCEIVE THE REASONS THAT LED KEATS TO ALTER THIS IN THE VERSION HE PUBLISHED IN LEIGH HUNT'S INDICATOR TO AH WHAT CAN AIL THEE WRETCHED WIGHT AND SO ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THAT THEY HAD TRAITOROUSLY ENDEAVORED TO SUBVERT THE FUNDAMENTAL LAWS AND GOVERNMENT OF THE KINGDOM TO DEPRIVE THE KING OF HIS REGAL POWER AND TO IMPOSE ON HIS SUBJECTS AN ARBITRARY AND TYRANNICAL AUTHORITY THAT THEY HAD ENDEAVORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THOUGH EVEN YET THERE WAS A SECOND HURRIEDLY APPREHENSIVE GLANCE TOWARD THE DOOR BEFORE SHE SPOKE THE LOWESTOFT YES I'M SO GLAD THAT IS OF COURSE I MUST BE GLAD I'LL GET IT HER VOICE BROKE AS SHE PULLED HERSELF FROM HER CHAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I WANT YOU TO ASK HER WHAT SHE THOUGHT OF THE PARTY I GAVE ON SUNDAY LAST DID YOU HOLD YOUR TONGUE AND DO AS I TELL YOU SHE'LL BE JEALOUS AND ASK YOU WHETHER YOU WERE PRESENT OF COURSE YOU WEREN'T FOR THERE WAS NO PARTY YOU'LL BOTH EXPRESS DISCONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AND I HOPE THAT MY READERS ARE NOW READY TO CONCLUDE WITH ME THAT THE PRETENDED SPIRITUALITY OF OUR EMOTIONS AND OF OUR ATTRIBUTES OF VALUE SO FAR FROM PROVING AN OBJECTION TO THE PHILOSOPHY OF PURE EXPERIENCE DOES WHEN RIGHTLY DISCUSSED AND ACCOUNTED FOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BUT OF COURSE IF THE HYPNOTIZER LOOKS STEADILY INTO THE EYES OF HIS SUBJECT AND THE SUBJECT LOOKS INTO HIS EYES THE STEADY GAZE ON A BRIGHT OBJECT WILL PRODUCE HYPNOTISM IN ONE QUITE AS READILY AS IN THE OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BUT WHEN SHE ASKED FOR SOMETHING THAT MEANT MORE THAN LIFE TO HER IT WAS REFUSED OF COURSE SHE HAD GONE THROUGH ALL SORTS OF HUMILIATION TO GET HIM THAT MONEY AND THIS WAS THE GRATITUDE SHE RECEIVED GRAHAM LISTENED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AND WE NEED NOT BE APPREHENSIVE THAT THERE WILL BE TOO MUCH STABILITY WHILE THERE IS EVEN THE OPTION OF CHANGING NOR NEED WE DESIRE TO PROHIBIT THE PEOPLE FROM CONTINUING THEIR CONFIDENCE WHERE THEY THINK IT MAY BE SAFELY PLACED AND WHERE BY CONSTANCY ON THEIR PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WERE HE LOVER HUSBAND OR SON SO AMABEL BELIEVED AND SO WOULD THESE OTHERS BELIEVE ALSO WHEN ONCE RELIEVED OF THE MAGNETIC PERSONALITY OF THIS EXTRAORDINARY WITNESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"].remove_columns([\"file\", \"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForPreTraining,\n",
    "    get_scheduler,\n",
    "    is_wandb_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"facebook/wav2vec2-base-960h\" #\"facebook/wav2vec2-large-lv60\" #\"facebook/wav2vec2-base-960h\" #\"patrickvonplaten/wav2vec2-base-v2\" \"facebook/wav2vec2-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option1\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "#option2\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.do_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "#option1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "#option2\n",
    "# tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# make sure that dataset decodes audio with correct sampling rate\n",
    "raw_datasets = raw_datasets.cast_column(\n",
    "    \"audio\", datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max & min audio length in number of samples\n",
    "max_duration_in_seconds = 5 \n",
    "min_duration_in_seconds = 3   \n",
    "max_length = int(max_duration_in_seconds * feature_extractor.sampling_rate) #80000\n",
    "min_length = int(min_duration_in_seconds * feature_extractor.sampling_rate) #48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(batch):\n",
    "#     sample = batch[\"audio\"]\n",
    "\n",
    "#     inputs = feature_extractor(\n",
    "#         sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], max_length=max_length, truncation=True\n",
    "#     )\n",
    "#     batch[\"input_values\"] = inputs.input_values[0]\n",
    "#     batch[\"input_length\"] = len(inputs.input_values[0])\n",
    "\n",
    "#     return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor(): #redirected to Wav2Vec2CTCTokenizer\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When wrapping the processor into the as_target_processor context, however, the same method is redirected to Wav2Vec2CTCTokenizer's call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/28254 [00:00<?, ? examples/s]c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 28254/28254 [04:45<00:00, 99.05 examples/s] \n",
      "Map: 100%|██████████| 285/285 [00:02<00:00, 99.04 examples/s] \n"
     ]
    }
   ],
   "source": [
    "cache_file_names = None\n",
    "vectorized_datasets = raw_datasets.map(\n",
    "        prepare_dataset,\n",
    "        num_proc=1,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        cache_file_names=cache_file_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 28254/28254 [00:00<00:00, 466308.85 examples/s]\n",
      "Filter: 100%|██████████| 285/285 [00:00<00:00, 22991.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#filter all sequences that are longer than 4 seconds out of the training dataset.\n",
    "if max_length > 0.0:\n",
    "    vectorized_datasets = vectorized_datasets.filter(\n",
    "        lambda x: x < max_length,\n",
    "        num_proc=1,\n",
    "        input_columns=[\"input_length\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'input_length', 'labels'],\n",
       "    num_rows: 1930\n",
       "})"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24,\n",
       " 16,\n",
       " 6,\n",
       " 4,\n",
       " 22,\n",
       " 8,\n",
       " 16,\n",
       " 4,\n",
       " 14,\n",
       " 10,\n",
       " 14,\n",
       " 9,\n",
       " 27,\n",
       " 6,\n",
       " 4,\n",
       " 20,\n",
       " 8,\n",
       " 13,\n",
       " 21,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 11,\n",
       " 5,\n",
       " 13,\n",
       " 4,\n",
       " 7,\n",
       " 16,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 17,\n",
       " 7,\n",
       " 16,\n",
       " 14,\n",
       " 4,\n",
       " 10,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 13,\n",
       " 23,\n",
       " 8,\n",
       " 12,\n",
       " 5,\n",
       " 14,\n",
       " 4,\n",
       " 18,\n",
       " 10,\n",
       " 6,\n",
       " 11,\n",
       " 4,\n",
       " 7,\n",
       " 15,\n",
       " 17,\n",
       " 8,\n",
       " 12,\n",
       " 6,\n",
       " 4,\n",
       " 17,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 19,\n",
       " 10,\n",
       " 9,\n",
       " 21,\n",
       " 4,\n",
       " 7,\n",
       " 13,\n",
       " 19,\n",
       " 11,\n",
       " 9,\n",
       " 5,\n",
       " 12,\n",
       " 12]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_datasets['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 1930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a data collator. In contrast to most NLP models, Wav2Vec2 has a much larger input length than output length. E.g., a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning Wav2Vec2 requires a special padding data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "#ref from https://github.com/huggingface/transformers/blob/main/examples/research_projects/wav2vec2/run_asr.py\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages (from jiwer) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading rapidfuzz-3.6.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 13.0 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.3 rapidfuzz-3.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1925: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./output/wav2vec2-finetune-asr\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valkeyname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=vectorized_datasets[\"train\"],\n",
    "    eval_dataset=vectorized_datasets[valkeyname],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7260' max='7260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7260/7260 55:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>0.078168</td>\n",
       "      <td>0.055046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.086693</td>\n",
       "      <td>0.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.226649</td>\n",
       "      <td>0.073394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.054149</td>\n",
       "      <td>0.073394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.146641</td>\n",
       "      <td>0.082569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.137821</td>\n",
       "      <td>0.082569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.299412</td>\n",
       "      <td>0.064220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>0.156699</td>\n",
       "      <td>0.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.085636</td>\n",
       "      <td>0.045872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.166695</td>\n",
       "      <td>0.073394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.200265</td>\n",
       "      <td>0.082569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.146623</td>\n",
       "      <td>0.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.064220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.126968</td>\n",
       "      <td>0.045872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7260, training_loss=0.11090213168751109, metrics={'train_runtime': 3319.9982, 'train_samples_per_second': 17.44, 'train_steps_per_second': 2.187, 'total_flos': 1.8826918008430003e+18, 'train_loss': 0.11090213168751109, 'epoch': 30.0})"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text'],\n",
       "    num_rows: 285\n",
       "})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facebook/wav2vec2-large-lv60\n",
    "from transformers import AutoProcessor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "sampling_rate = raw_datasets[\"test\"].features[\"audio\"].sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "eval_processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "eval_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:\\\\Users\\\\lkk68\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\a77e60630727409949078da012aba02689008a522ed19f5f43300c735482710c\\\\103-1240-0000.flac',\n",
       " 'audio': {'path': 'C:\\\\Users\\\\lkk68\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\a77e60630727409949078da012aba02689008a522ed19f5f43300c735482710c\\\\103-1240-0000.flac',\n",
       "  'array': array([-0.00650024, -0.00552368, -0.00619507, ...,  0.0032959 ,\n",
       "          0.00045776, -0.00946045]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK',\n",
       " 'speaker_id': 103,\n",
       " 'chapter_id': 1240,\n",
       " 'id': '103-1240-0000'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = eval_processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "inputs=inputs.to('cuda')\n",
    "with torch.no_grad():\n",
    "    logits = eval_model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_str = eval_processor.batch_decode(predicted_ids)[0]\n",
    "pred_str\n",
    "#text = eval_processor.decode(dataset[0][\"text\"], group_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transcribe speech\n",
    "transcription = eval_processor.batch_decode(predicted_ids)\n",
    "transcription[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"labels\"] = eval_processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[-0.3012, -0.2587, -0.2879,  ...,  0.1243,  0.0010, -0.4297]],\n",
       "       device='cuda:0'), 'labels': tensor([[19, 11,  7, 23,  6,  5, 13,  4,  8,  9,  5,  4, 17, 10, 12, 12, 16, 12,\n",
       "          4, 13,  7, 19, 11,  5, 15,  4, 15, 22,  9, 14,  5,  4, 10, 12,  4, 12,\n",
       "         16, 13, 23, 13, 10, 12,  5, 14,  4, 17, 10, 12, 12, 16, 12,  4, 13,  7,\n",
       "         19, 11,  5, 15,  4, 15, 22,  9, 14,  5,  4, 15, 10, 25,  5, 14,  4, 29,\n",
       "         16, 12,  6,  4, 18, 11,  5, 13,  5,  4,  6, 11,  5,  4,  7, 25,  8,  9,\n",
       "         15,  5,  7,  4, 17,  7, 10,  9,  4, 13,  8,  7, 14,  4, 14, 10, 23, 23,\n",
       "          5, 14,  4, 14,  8, 18,  9,  4, 10,  9,  6,  8,  4,  7,  4, 15, 10,  6,\n",
       "          6, 15,  5,  4, 11,  8, 15, 15,  8, 18,  4, 20, 13, 10,  9, 21,  5, 14,\n",
       "          4, 18, 10,  6, 11,  4,  7, 15, 14,  5, 13, 12,  4,  7,  9, 14,  4, 15,\n",
       "          7, 14, 10,  5, 12,  4,  5,  7, 13, 14, 13,  8, 23, 12,  4,  7,  9, 14,\n",
       "          4,  6, 13,  7, 25,  5, 13, 12,  5, 14,  4, 24, 22,  4,  7,  4, 24, 13,\n",
       "          8,  8, 26]])}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.73"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute loss, inputs as labels and input_values\n",
    "loss = eval_model(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate own trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Wav2Vec2ForCTC.from_pretrained(\".\\output\\wav2vec2-finetune-asr\\checkpoint-7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel=mymodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:53: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '.\\output\\wav2vec2-finetune-asr\\checkpoint-7000'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '.\\output\\wav2vec2-finetune-asr\\checkpoint-7000' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:51\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\processing_utils.py:226\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m--> 226\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\processing_utils.py:270\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m--> 270\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(attribute_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:748\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1838\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m-> 1838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1839\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1842\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1843\u001b[0m     )\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for '.\\output\\wav2vec2-finetune-asr\\checkpoint-7000'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '.\\output\\wav2vec2-finetune-asr\\checkpoint-7000' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[193], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m myprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2Processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mwav2vec2-finetune-asr\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoint-7000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:63\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a tokenizer inside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from a config that does not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m include a `tokenizer_class` attribute is deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m Wav2Vec2FeatureExtractor\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 63\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Wav2Vec2CTCTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(feature_extractor\u001b[38;5;241m=\u001b[39mfeature_extractor, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32mc:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1838\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1833\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1834\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1835\u001b[0m     )\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m-> 1838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1839\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1842\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1843\u001b[0m     )\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for '.\\output\\wav2vec2-finetune-asr\\checkpoint-7000'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '.\\output\\wav2vec2-finetune-asr\\checkpoint-7000' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "myprocessor = Wav2Vec2Processor.from_pretrained(\".\\output\\wav2vec2-finetune-asr\\checkpoint-7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.\\\\output\\\\wav2vec2-finetune-asr\\\\checkpoint-7000\\\\tokenizer_config.json',\n",
       " '.\\\\output\\\\wav2vec2-finetune-asr\\\\checkpoint-7000\\\\special_tokens_map.json',\n",
       " '.\\\\output\\\\wav2vec2-finetune-asr\\\\checkpoint-7000\\\\vocab.json',\n",
       " '.\\\\output\\\\wav2vec2-finetune-asr\\\\checkpoint-7000\\\\added_tokens.json')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\".\\output\\wav2vec2-finetune-asr\\checkpoint-7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "myprocessor = Wav2Vec2Processor.from_pretrained(\".\\output\\wav2vec2-finetune-asr\\checkpoint-7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='.\\output\\wav2vec2-finetune-asr\\checkpoint-7000', vocab_size=32, model_max_length=9223372036854775807, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='facebook/wav2vec2-base-960h', vocab_size=32, model_max_length=9223372036854775807, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "  with torch.no_grad():\n",
    "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "    logits = mymodel(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = myprocessor.batch_decode(pred_ids)[0]\n",
    "  batch[\"text\"] = myprocessor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "  return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will make use of the map(...) function to predict the transcription of every test sample and to save the prediction in the dataset itself. We will call the resulting dictionary \"results\".\n",
    "\n",
    "Note: we evaluate the test data set with batch_size=1 on purpose due to this issue. Since padded inputs don't yield the exact same output as non-padded inputs, a better WER can be achieved by not padding the input at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelresult=myprocessor.decode(vectorized_datasets[\"test\"][0]['labels'], group_tokens=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IT WAS IMPOSSIBLE TO RESIST AN EXISTENCE'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:01<00:00,  9.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "results = vectorized_datasets[\"test\"].map(map_to_result, remove_columns=vectorized_datasets[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the overall WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.046\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pred_str', 'text'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BE AT PRUDENCE'S TONIGHT AT EIGHT</td>\n",
       "      <td>BE AT PRUDENCE'S TO NIGHT AT EIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I HAPPENED TO DODGE A BALL FIRED FROM THE OTHER SIDE AND IT WENT THROUGH THE GLASS WHAT</td>\n",
       "      <td>I HAPPENED TO DODGE A BALL FIRED FROM THE OTHER SIDE AND IT WENT THROUGH THE GLASS WHAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I HAVE TRAVELLED A GOOD DEAL AND SEEN MUCH GRANDER THINGS</td>\n",
       "      <td>I HAVE TRAVELLED A GOOD DEAL AND SEEN MUCH GRANDER THINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALTHOUGH THE LATTER HAD NOT BEEN A RESIDENT THERE MUCH MORE THAN THREE YEARS</td>\n",
       "      <td>ALTHOUGH THE LATTER HAD NOT BEEN A RESIDENT THERE MUCH MORE THAN THREE YEARS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELL SO LONG BOYS AND WE ALL WISH YOU SUCCESS</td>\n",
       "      <td>WELL SO LONG BOYS AND WE ALL WISH YOU SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NO AT POINT DEJOUR WHERE WE HAD DINNER THE DUKE AND I</td>\n",
       "      <td>NO AT POINT DU JOUR WHERE WE HAD DINNER THE DUKE AND I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IT WAS IMPOSSIBLE TO RESIST IN EXISTENCE</td>\n",
       "      <td>IT WAS IMPOSSIBLE TO RESIST AN EXISTENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AS THE FOUR CHUMS WENT AWAY JERRY CHUCKLED</td>\n",
       "      <td>AS THE FOUR CHUMS WENT AWAY JERRY CHUCKLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OH NO IT ISN'T SO BAD AS THAT HE WAS ASSURED</td>\n",
       "      <td>OH NO IT ISN'T SO BAD AS THAT HE WAS ASSURED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IN THE COURSE OF THE DAY I RECEIVED THIS NOTE</td>\n",
       "      <td>IN THE COURSE OF THE DAY I RECEIVED THIS NOTE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> I I T <pad> | | W W A <pad> S S | | <pad> I I M <pad> <pad> <pad> P <pad> <pad> <pad> <pad> O S <pad> <pad> <pad> S S <pad> <pad> I I <pad> B <pad> L L L E | | <pad> <pad> T <pad> <pad> O O | | R R <pad> E <pad> <pad> <pad> S <pad> <pad> <pad> <pad> I I S S <pad> T T <pad> | | | <pad> <pad> I <pad> N <pad> | | | <pad> E E <pad> X <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> I I S S <pad> <pad> T T <pad> <pad> <pad> <pad> <pad> E N N <pad> <pad> C E E <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  logits = mymodel(torch.tensor(vectorized_datasets[\"test\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    vectorized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "learning_rate=5e-5\n",
    "adam_beta1=0.9\n",
    "adam_beta2=0.999\n",
    "adam_epsilon=1e-8\n",
    "optimizer = AdamW(\n",
    "    list(model.parameters()),\n",
    "    lr=learning_rate,\n",
    "    betas=[adam_beta1, adam_beta2],\n",
    "    eps=adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "gradient_accumulation_steps =1\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 6\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup_steps = 10\n",
    "lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", #[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculate our number of training epochs\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = batch_size * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpuid=0\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:'+str(gpuid))  # CUDA GPU 0\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 48/1452 [00:32<15:57,  1.47it/s]\n",
      " 17%|█▋        | 242/1452 [01:51<08:15,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.016273705288767815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 484/1452 [03:43<06:07,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.018542783334851265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 726/1452 [05:34<04:52,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 0.014463784173130989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 968/1452 [07:26<03:17,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 0.015551198273897171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1210/1452 [09:18<01:36,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 0.03556506335735321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1452/1452 [11:09<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 0.04022493213415146\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(max_train_steps))\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    losses=[]\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        #backward\n",
    "        loss.backward()\n",
    "\n",
    "        if step % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "        \n",
    "    print(f\"epoch {epoch}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    vectorized_datasets[valkeyname], collate_fn=data_collator, batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch=next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47360])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(model, processor, eval_dataloader, device):\n",
    "    # Evaluation\n",
    "    totallen = len(eval_dataloader)\n",
    "    print(\"Total evaluation length:\", totallen)\n",
    "    model.eval()\n",
    "    predictions=[]\n",
    "    references=[]\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            #outputs = model(**batch)\n",
    "            print(\"input shape:\", batch[\"input_values\"].shape)\n",
    "            input_values = torch.tensor(batch[\"input_values\"], device=device)#.unsqueeze(0)\n",
    "            logits = model(input_values).logits\n",
    "        #logits = outputs.logits\n",
    "        print(logits.shape)\n",
    "        predicted_ids = torch.argmax(logits[0], dim=-1)\n",
    "        # transcribe speech\n",
    "        print(\"predicted_ids shape\", predicted_ids.shape)\n",
    "        #print(predicted_ids)\n",
    "        pred_str = processor.decode(predicted_ids)\n",
    "        #batch_decode results do not perform tokenization?\n",
    "        predictions.append(pred_str)\n",
    "        text_str = processor.decode(batch[\"labels\"][0], group_tokens=False)\n",
    "        references.append(text_str)\n",
    "        print(\"pred_str\", pred_str)\n",
    "        print(\"text_str\", text_str)\n",
    "    werscore=wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(\"Test WER: {:.3f}\".format(werscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluation length: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 47360])\n",
      "torch.Size([1, 147, 32])\n",
      "predicted_ids shape torch.Size([147])\n",
      "pred_str IT WAS IMPOSSIBLE TO RESIST IN EXISTENCE\n",
      "text_str IT WAS IMPOSSIBLE TO RESIST AN EXISTENCE\n",
      "input shape: torch.Size([1, 64320])\n",
      "torch.Size([1, 200, 32])\n",
      "predicted_ids shape torch.Size([200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lkk68\\AppData\\Local\\Temp\\ipykernel_15572\\2957877289.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = torch.tensor(batch[\"input_values\"], device=device)#.unsqueeze(0)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_str I HAVE TRAVELLED A GOOD DEAL AND SEEN MUCH GRANDER THINGS\n",
      "text_str I HAVE TRAVELLED A GOOD DEAL AND SEEN MUCH GRANDER THINGS\n",
      "input shape: torch.Size([1, 52640])\n",
      "torch.Size([1, 164, 32])\n",
      "predicted_ids shape torch.Size([164])\n",
      "pred_str IN THE COURSE OF THE DAY I RECEIVED THIS NOTE\n",
      "text_str IN THE COURSE OF THE DAY I RECEIVED THIS NOTE\n",
      "input shape: torch.Size([1, 44960])\n",
      "torch.Size([1, 140, 32])\n",
      "predicted_ids shape torch.Size([140])\n",
      "pred_str BE AT PRUDENCE'S TONIGHT AT EIGHT\n",
      "text_str BE AT PRUDENCE'S TO NIGHT AT EIGHT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:00,  9.76it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 66320])\n",
      "torch.Size([1, 207, 32])\n",
      "predicted_ids shape torch.Size([207])\n",
      "pred_str NO AT POINT DEJOUR WHERE WE HAD DINNER THE DUKE AND I\n",
      "text_str NO AT POINT DU JOUR WHERE WE HAD DINNER THE DUKE AND I\n",
      "input shape: torch.Size([1, 76320])\n",
      "torch.Size([1, 238, 32])\n",
      "predicted_ids shape torch.Size([238])\n",
      "pred_str I HAPPENED TO DODGE A BALL FIRED FROM THE OTHER SIDE AND IT WENT THROUGH THE GLASS WHAT\n",
      "text_str I HAPPENED TO DODGE A BALL FIRED FROM THE OTHER SIDE AND IT WENT THROUGH THE GLASS WHAT\n",
      "input shape: torch.Size([1, 61840])\n",
      "torch.Size([1, 193, 32])\n",
      "predicted_ids shape torch.Size([193])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_str ALTHOUGH THE LATTER HAD NOT BEEN A RESIDENT THERE MUCH MORE THAN THREE YEARS\n",
      "text_str ALTHOUGH THE LATTER HAD NOT BEEN A RESIDENT THERE MUCH MORE THAN THREE YEARS\n",
      "input shape: torch.Size([1, 44560])\n",
      "torch.Size([1, 139, 32])\n",
      "predicted_ids shape torch.Size([139])\n",
      "pred_str AS THE FOUR CHUMS WENT AWAY JERRY CHUCKLED\n",
      "text_str AS THE FOUR CHUMS WENT AWAY JERRY CHUCKLED\n",
      "input shape: torch.Size([1, 52000])\n",
      "torch.Size([1, 162, 32])\n",
      "predicted_ids shape torch.Size([162])\n",
      "pred_str OH NO IT ISN'T SO BAD AS THAT HE WAS ASSURED\n",
      "text_str OH NO IT ISN'T SO BAD AS THAT HE WAS ASSURED\n",
      "input shape: torch.Size([1, 54240])\n",
      "torch.Size([1, 169, 32])\n",
      "predicted_ids shape torch.Size([169])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_str WELL SO LONG BOYS AND WE ALL WISH YOU SUCCESS\n",
      "text_str WELL SO LONG BOYS AND WE ALL WISH YOU SUCCESS\n",
      "Test WER: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(mymodel, processor, eval_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondapy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
